def dedupe_from_ytd(config, mode, current_month_df):
    """
    Def:
        Pull all medicaid data for source year to date (Jan 1st - last day of 2 months prior to current
        date). Remove duplicates from current_month_df using year to date data. Returns unique data and
        duplicate data in two seperate dataframes.

    Args:
        config: dict-like, contains connection/sql/filter settings.
        mode: 'previous_month', 'data_refresh', or 'sample'.
        current_month_df: DataFrame to dedupe (the "left" side).

    Returns:
        final_unique: DF with all duplicate records removed (same schema as current_month_df).
        final_duplicate: DF containing removed duplicate records (same schema + 'Fallout Reason').
    """
    print('Removing Duplicates...')

    server = config['connection']['server']
    db_name = config['connection']['database']
    raw_sql = config['sql_dedupe']
    dedupe_columns = config['filter_rules']['dedupe_columns']
    summary_data['DeDupe_Columns'] = dedupe_columns  # preserve side-effect

    # Preserve your top_clause logic exactly
    if mode == "previous_month":
        top_clause = ""
    elif mode == "data_refresh":
        top_clause = ""
    elif mode == "sample":
        top_clause = "TOP 1000"

    # ----------------- MEMORY-LIGHT BRANCH -----------------
    if mode == "previous_month" or mode == "sample":
        # Same YTD date logic you intended (dynamic year start)
        start_date = date(date.today().year, 1, 1)
        end_date = (date.today().replace(day=1) - timedelta(days=1)) - relativedelta(months=1)

        sql_query = (
            raw_sql
            .replace('{top_clause}', top_clause)
            .replace('{start_date}', str(start_date))
            .replace('{end_date}', str(end_date))
        )
        logging.info(
            f"===== Checking for duplicates from data between {start_date} and {end_date} ====="
        )

        conn = (
            'mssql+pyodbc://' + server + '/' + db_name + '?driver=SQL+Server+Native+Client+11.0'
        )
        print(sql_query)
        query = sql_query

        # Keep output schema identical to the left/current dataset
        left_cols = list(current_month_df.columns)

        # Work on a local copy
        current_df = current_month_df.copy()

        # Ensure consistent dtypes on dedupe keys (strings except DOS) on LEFT
        for c in dedupe_columns:
            if c != 'DOS' and c in current_df.columns:
                current_df[c] = current_df[c].astype('string')

        # Helper: build a hash key over dedupe_columns (stable & compact)
        def _build_key(df, cols):
            parts = {}
            for col in cols:
                if col == 'DOS':
                    # Keep DOS as datetime for hashing
                    parts[col] = df[col]
                else:
                    parts[col] = df[col].astype('string').fillna('')
            key_df = pd.DataFrame(parts)
            return pd.util.hash_pandas_object(key_df, index=False)

        # Precompute LEFT keys once
        left_key = _build_key(current_df, dedupe_columns)
        # Boolean mask marking rows we still consider "unique/unused"
        alive = pd.Series(True, index=current_df.index)

        # Stream YTD in smaller chunks to reduce peak memory
        for chunk in pd.read_sql(
            query,
            conn,
            coerce_float=False,
            parse_dates=['DOS'],
            chunksize=100_000,  # smaller than before to relieve pressure
        ):
            print('Processing Chunk....')

            # Optional ICD split (unchanged behavior)
            try:
                icd_split = config['custom_logic']['icd_split']
                icd_col_name = config['custom_logic']['icd_params']['icd_col_name']
                icd_range = config['custom_logic']['icd_params']['icd_range']
                code_cols = config['filter_rules']['code_columns']
                meta_cols = ['DOS', 'LOINC', 'Result', 'SNOMED', 'MedicaidID', 'CPTPx', 'HCPCSPx']
            except Exception:
                icd_split = False

            if icd_split:
                print('Inside ICD SPLIT Function')
                # NOTE: normalize_icd_codes can blow up rows if many diags exist.
                # We keep it to preserve behavior; if memory still spikes, consider
                # turning icd_split OFF only for dedupe (config flag) or pushing
                # UNPIVOT to SQL with a WHERE to drop blanks.
                chunk = normalize_icd_codes(chunk, code_cols, icd_col_name, icd_range, meta_cols)

            # Match dtypes on dedupe keys on RIGHT
            for c in dedupe_columns:
                if c != 'DOS' and c in chunk.columns:
                    chunk[c] = chunk[c].astype('string')

            # Build a set of RIGHT keys for this chunk.
            # IMPORTANT: avoid .unique() to save a big temporary arrayâ€”set() will dedupe anyway.
            chunk_key = _build_key(chunk, dedupe_columns)
            key_set = set(chunk_key.dropna())

            # Only check rows still alive (saves work on later chunks)
            is_dup = alive & left_key.isin(key_set)

            # Immediately mark them as not-alive; we don't accumulate any other arrays
            if is_dup.any():
                alive.loc[ is_dup ] = False

            # Free memory ASAP
            del chunk, chunk_key, key_set, is_dup
            import gc; gc.collect()

        # After processing all chunks:
        #  - rows with alive==True are unique
        #  - rows with alive==False are duplicates
        final_unique = current_df.loc[alive, left_cols].copy()
        final_duplicate = current_df.loc[~alive, left_cols].copy()

        # Tag fallout reason, consistent with your pipeline
        if not final_duplicate.empty:
            final_duplicate['Fallout Reason'] = 'Duplicate Record'
        else:
            # ensure column exists for empty frame
            final_duplicate['Fallout Reason'] = pd.Series([], dtype='string')

        print(f"Real Duplicate Count: {len(final_duplicate)}")
        return final_unique, final_duplicate

    # ----------------- UNCHANGED BRANCH -----------------
    if mode == "data_refresh":
        subset_columns = dedupe_columns
        current_df = current_month_df.copy()

        duplicates = current_df[current_df.duplicated(subset=subset_columns, keep='first')].copy()
        duplicates['Fallout Reason'] = 'Duplicate Record'
        print(f"Real Duplicate Count: {len(duplicates)}")

        main_df = current_df.drop_duplicates(subset=subset_columns, keep='first')

        return main_df, duplicates
